{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "# **Elements of Fully Connected Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "- Understand the elements of a perceptron and a multi-layer perceptron (feed forward NN)\n",
    "- Understand how to setup a fully connected neural network\n",
    "- Understand practices in applied machine learning: k-fold cross-validation and regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a *fully-connected neural network*:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/dense_nn.png\" width=\"300\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can allow us to learn complex non-linear mappings from an input to the output space, when large and informative data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron/Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is made up of layers of nodes. Each layer has W nodes, where W is known as the width of the neural network. The layers between the input and output layers are known as hidden layers.\n",
    "\n",
    "A node computes a weighted sum of its inputs; it applies **weights** to each input, and includes an additional **bias** term. Then it applies an activation function to this term.\n",
    "\n",
    "$$\n",
    "y_i = f\\left(b_i + \\sum_{j=1}^{N} x_j w_{ij} \\right)\n",
    "$$\n",
    "\n",
    "where,\n",
    "- **$y_i$** → Output of the node (perceptron) after applying the activation function.  \n",
    "- **$f(\\cdot)$** → Activation function that introduces non-linearity (e.g., ReLU, sigmoid).  \n",
    "- **$\\sum_{j=1}^{N} x_j w_{ij}$** → Weighted sum of inputs before activation.  \n",
    "- **$x_j$** → Input feature \\( j \\) to the node.  \n",
    "- **$w_{ij}$** → Weight associated with input \\( x_j \\) for node \\( i \\).  \n",
    "- **$N$** → Total number of input features to the node.\n",
    "\n",
    "The values of the weights and biases of each node in a NN is learned by minimising an error metric.\n",
    "\n",
    "$$\n",
    "(\\mathbf{w}_{opt}, b_{opt}) = \\arg\\min_{\\mathbf{w}, b} L(\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}, b } \\; L(\\mathbf{w}, b) = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative method used to find the minima of a function. The idea is to start at a random point, and move in the negative gradient direction. We keep moving in this direction until we reach a 0 gradient position, which is the minima. Note, this method will find a local minima, but not necessarily the global minima. Futhermore, the learning rate controls how big your step is in the negative direction. If this is too big, we might overshoot the minimum. If it is too small, it will take a lot more time to reach the minima. \n",
    "\n",
    "Say that we are trying to find the value of $x$ that minimises the function $f(x)$. At each step of the algorithm, we update the paramater using the formula:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} - \\alpha \\nabla f(x_{t-1})\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $x_t$ is the parameter at iteration $t$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\nabla f (x_{t-1})$ is the gradient of the function at the current point\n",
    "\n",
    "The iterative process continues until the algorithm converges at a minimum point, where the gradient becomes very small (close to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/gradient_descent.jpg\" width=\"500\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we solve the weights and bias optimisation problem using gradient descent. The gradient descent equations for neural networks are:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t} = \\mathbf{w}_{t-1} - \\alpha \\nabla_{\\mathbf{w}} L(\\mathbf{w}_{t-1}, b_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{t} = b_{t-1} - \\alpha \\nabla_b L(\\mathbf{w}_{t-1}, b_{t-1})\n",
    "$$\n",
    "\n",
    "The **back propogation algorithm** in a neural network uses gradient descent to adjust weights and biases, minimising the cost function. It applies the chain rule to propagate errors backwards through the layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multi-layer perceptron (MLP), nodes are organised in layers. The first layer is known as the input layer; this is where the features/raw data will be. The final layer is known as the output layer. The layers in-between are known as hidden layers. In a fully connected neural network, also known as a dense neural network, each neuron is fully connected to all neurons in the previous and subsequent layers. Note, in an MLP, neurons in a single layer are independent. \n",
    "\n",
    "We can alter the NN architecture by altering any of the following hyperparameters:\n",
    "- number of nodes in output layer\n",
    "- number of nodes in input layer\n",
    "- number of nodes in each hidden layer\n",
    "- activation function for the hidden layers\n",
    "- activation function for the output layer\n",
    "- performance metric (loss function)\n",
    "\n",
    "Once we have defined the NN architecture, then we can train the model to find the optimal weight and bias values. The model's performance can vary significantly depending on the choice of NN architecture. We should tune this hyperparameters for optimal model performance. \n",
    "\n",
    "A deep neural network has many hidden layers. Advantages of deep neural networks include:\n",
    "- the ability to represent complex mappings\n",
    "- eliminates the need of domain expertise and feature engineering\n",
    "- can learn high-level features (complex or abstract) from data in an incremental way\n",
    "\n",
    "Some applications require more complex neural network architectures than a dense neural network. Some examples are natural language processing, speech recognitition and image classification.\n",
    "\n",
    "The problem with neural networks are that they lack interpretability. We don't know why a model succeeds or fails. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are also called transfer functions or squashing functions.The activation function transforms the weighted input, producing the node's output. They are responsible for introducing non-linearity into the network. Additionaly, they are differentiable. This is important for back propagation. These functions can be continous or discontinous.\n",
    "\n",
    "**Rectified Linear Activation (ReLU)**\n",
    "- Most common\n",
    "- Used for MLP and convolutional NN\n",
    "- Less susceptible to vanishing gradients that prevent deep models from being trained\n",
    "- *Good practice* -> use a 'He Normal' or 'He Uniform' weight intialisation, normalise input to the range (0-1) prior to training\n",
    "\n",
    "**Logistic (Sigmoid)**\n",
    "- Used in recurrent NN\n",
    "- Good for binary or multilabel classification\n",
    "- *Good practice* -> to use a 'Xavier Normal' or 'Xavier Uniform' weight initialisation, normalise intput to the range (0-1) prior to training\n",
    "\n",
    "**Hyperbolic Tangent (Tanh)**\n",
    "- Used in recurrent NN\n",
    "- Very similar to the sigmoid\n",
    "- Takes any real valu as input and output values in the range (-1, 1)\n",
    "- *Good practice* -> to use a 'Xavier Normal' or 'Xavier Uniform' weight initialisation, scale input to the range (-1,1) prior to training \n",
    "\n",
    "**Linear**\n",
    "- For regression task\n",
    "- Returns the value directly\n",
    "- *Good practice* -> models with linear activation function in the output layer are typically scaled prior to modeling using normalisation or standardisation transforms\n",
    "\n",
    "**Softmax**\n",
    "- This is just the generalisation of the logistice function to multiple dimensions. \n",
    "- Used in multinomial refression and multiclass classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/activation_functions.png\" width=\"600\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap of network training:\n",
    "\n",
    "1) Define neural network architecture\n",
    "2) Start with random initial parameters for weights and biases\n",
    "3) In each iteration of the training process:\n",
    "    - Forward propogation -> apply input data to neural network and obtain the output for the current parameter values.\n",
    "    - Backward propogation -> utilise gradient descent to adjust weights and biases to minimise loss function. Typically, implemented using automatic differentiation\n",
    "4) Repeat number 3 for a predefined number of iterations (also known as epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sizes and Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch size** is the number of training samples to work through before the error is calculated and the model's internal parameters are updated.\n",
    "\n",
    "**Number of epochs** is the number of complete passes through the training dataset. So, 1 epoch iscomprised of 1 or more batches. So, after 1 epoch, each sample in the training dataset has had an opportunity to update the internal model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/batches_and_epochs.png\" width=\"400\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent (SDG)**\n",
    "\n",
    "*Batch size: 1 sample*\n",
    "- Frequent updates\n",
    "- Gives immediate insight on performance and rate of improvement\n",
    "- Can result in a noisy gradient signal (may cause model paramaters and model error to jump aroung)\n",
    "- Computationally expensive for large datasets\n",
    "\n",
    "**Batch Gradient Descent (BGD)**\n",
    "\n",
    "*Batch size: all training samples*\n",
    "- Fewer updates\n",
    "- More stable error in the gradient, which may result in a more stable convergence\n",
    "- More computationally efficient\n",
    "- Can easily be parallelised (separating the calculation of prediction errors and the model update)\n",
    "\n",
    "**Mini-Batch Gradient Descent (M-BGD)**\n",
    "\n",
    "*Batch size: > 1 sample, < all training samples*\n",
    "- More frequent model update than BGS, this should help avoid local minimas\n",
    "- Error information is accumulated across mini-batches (as in BGD)\n",
    "- Small mini-batch size (MBS) -> learning process converges quickly at the cost of noise in the training process. This is because althought there are more frequent updates, each mini-batch has a higher variance, making weight updates less stable and potentially causing fluctuations in the learning process.\n",
    "- Large mini-batch size (MBS) -> learning process converges slowly with accurate estimates in the error gradient\n",
    "- Choose the mini-batch size by balancing fast convergence with stable performance.\n",
    "- Usually we pick sizes that are power of 2 for GPU/CPU hardware. Sizes like 32, 64 or 128. This may require discarding some samples so that the dataset can divide evenly into the mini-batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training a model, we need to choose the initial parameters for the weights and biases. We usually set these values to small random values to avoid getting stuck in a local minima. Each input to a node should have different weight initialisations. Usually, the choice of a particular initialisation model depends on the chosen activation function from the hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is the amount that the weights are updated during training. This can be constant or decrease with time. A higher learning rate speeds up learning but may cause instability. A lower learning rate ensures more precised updates but can slow down convergence. If we look back at the gradient descent algorithm, the $\\mathbf{\\beta}$ is the **learning rate**.\n",
    "\n",
    "Note, SGD uses a single learning rate for all weight updates. This may be constant throughout training, or you can use learning rate schedules to adjust it over time. Algorithms like Adam or Adagrad automatically adjust the learning rate for each parameter based on past gradients, to help speed up convergence and improve stability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a trained model is evaluated using a validation set, which help fine-tune hyperparameters for optimal reslts. Key hyperparameters are the number of hidden layers, nodes per layer and the activation functions. We have to also ensure we balance model complexity so it is'nt underfitting or overfitting. \n",
    "\n",
    "We can try search for the best choice of hyperparameters using different search strategies:\n",
    "- Random\n",
    "- Grid\n",
    "- Heuristic\n",
    "- Exhaustive (only feasible for small networks and datasets)\n",
    "\n",
    "**k-folds cross validation**\n",
    "This is a technique used to evaluate a machine learning model's performance while reducing variance and making better use of available data. Instead of using a single validation set, the dataset is split into k equally sized subsets. The model is trained k times, each time using k-1 folds for training and the remaing 1 fold for validation. This process is repeated k times, with each fold serving as the validation set. The final performance score is the average of all k iterations, fiving a more reliable estimate of model's performance. So, 5-fold cross validation means that for each iteration, 4 folds are used for training and 1 is used for validation. We repeat this 5 times, each time using a different fold for validation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
