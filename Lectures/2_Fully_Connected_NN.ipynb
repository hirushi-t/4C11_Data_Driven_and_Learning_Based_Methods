{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "# **Elements of Fully Connected Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "- Understand the elements of a perceptron and a multi-layer perceptron (feed forward NN)\n",
    "- Understand how to setup a fully connected neural network\n",
    "- Understand practices in applied machine learning: k-fold cross-validation and regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a *fully-connected neural network*:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/dense_nn.png\" width=\"300\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can allow us to learn complex non-linear mappings from an input to the output space, when large and informative data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron/Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is made up of layers of nodes. Each layer has W nodes, where W is known as the width of the neural network. The layers between the input and output layers are known as hidden layers.\n",
    "\n",
    "A node computes a weighted sum of its inputs; it applies **weights** to each input, and includes an additional **bias** term. Then it applies an activation function to this term.\n",
    "\n",
    "$$\n",
    "y_i = f\\left(b_i + \\sum_{j=1}^{N} x_j w_{ij} \\right)\n",
    "$$\n",
    "\n",
    "where,\n",
    "- **$y_i$** → Output of the node (perceptron) after applying the activation function.  \n",
    "- **$f(\\cdot)$** → Activation function that introduces non-linearity (e.g., ReLU, sigmoid).  \n",
    "- **$\\sum_{j=1}^{N} x_j w_{ij}$** → Weighted sum of inputs before activation.  \n",
    "- **$x_j$** → Input feature \\( j \\) to the node.  \n",
    "- **$w_{ij}$** → Weight associated with input \\( x_j \\) for node \\( i \\).  \n",
    "- **$N$** → Total number of input features to the node.\n",
    "\n",
    "The values of the weights and biases of each node in a NN is learned by minimising an error metric.\n",
    "\n",
    "$$\n",
    "(\\mathbf{w}_{opt}, b_{opt}) = \\arg\\min_{\\mathbf{w}, b} L(\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}, b } \\; L(\\mathbf{w}, b) = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative method used to find the minima of a function. The idea is to start at a random point, and move in the negative gradient direction. We keep moving in this direction until we reach a 0 gradient position, which is the minima. Note, this method will find a local minima, but not necessarily the global minima. Futhermore, the learning rate controls how big your step is in the negative direction. If this is too big, we might overshoot the minimum. If it is too small, it will take a lot more time to reach the minima. \n",
    "\n",
    "Say that we are trying to find the value of $x$ that minimises the function $f(x)$. At each step of the algorithm, we update the paramater using the formula:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} - \\alpha \\nabla f(x_{t-1})\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $x_t$ is the parameter at iteration $t$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\nabla f (x_{t-1})$ is the gradient of the function at the current point\n",
    "\n",
    "The iterative process continues until the algorithm converges at a minimum point, where the gradient becomes very small (close to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/gradient_descent.jpg\" width=\"500\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we solve the weights and bias optimisation problem using gradient descent. The gradient descent equations for neural networks are:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t} = \\mathbf{w}_{t-1} - \\alpha \\nabla_{\\mathbf{w}} L(\\mathbf{w}_{t-1}, b_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{t} = b_{t-1} - \\alpha \\nabla_b L(\\mathbf{w}_{t-1}, b_{t-1})\n",
    "$$\n",
    "\n",
    "FROM HERE, PAGE 3 LECTURE 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
