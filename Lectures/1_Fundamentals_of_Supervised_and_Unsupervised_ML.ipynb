{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fundamentals of Supervised and Unsupervised ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Understand the importance of ML in mechanics and materials\n",
    "- Understand what we need to learn a model\n",
    "- Understand how to use learning curves to asses model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any ML model, it is important that we have the the **right type of data**. This means we have to ensure it is relevant and of a high-quality. We need to ensure it isn't biased in any way.\n",
    "\n",
    "The **manifold hypothesis** states that many real-world high-dimensional datasets actually lie along a low-dimensional latent manifold inside that high-dimensional space. \n",
    "\n",
    "ML algorithms can learn this low-dimensional structure of data, which is something that would be impossible for humans to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning Algo Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Gather Data**  \n",
    "   Collect data from relevant sources.\n",
    "\n",
    "2. **Data Processing & Cleaning**  \n",
    "   Preprocess the data, handle missing values, and clean it for analysis.\n",
    "\n",
    "3. **Build the Model**  \n",
    "   Choose an appropriate machine learning algorithm and train the model on the data.\n",
    "\n",
    "4. **Extract Insights**  \n",
    "   Analyse the model's results. What does it tell us about the data?\n",
    "\n",
    "5. **Data Visualization**  \n",
    "   Visualize the findings to communicate insights effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning** is the most important and longest step in this process. Here are some important processes in cleaning a dataset.\n",
    "- *Data standardisation* -> convert data into the same format (same units, remove punctuation etc)\n",
    "- *Removing unwanted observations* -> get rid of duplicates or redundant data. Consider what is 'valid' for your model\n",
    "- *Handling missing data* -> dealing with unknown data (e.g. NaN). You may ignore them, set them to 0 or try to predict them\n",
    "- *Structural error solving* -> errors in the setup of the data (e.d. mislabelled classes)\n",
    "- *Outliers' management* -> dealing with values that don't belong in out dataset (e.g. we might solve this by defining min and max values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data in materials and mechanics applications** can be:\n",
    "- expensive to obtain\n",
    "- difficult to measure\n",
    "- noisy (if its obtained from measurements)\n",
    "- deterministic (if its simulated)\n",
    "- heterogenous (comed from different sources)\n",
    "- multi-modal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering is the process of transforming raw data into features that are suitable for machine learning models**. \n",
    "\n",
    "For example, in a dataset of housing prices, features could include the number of bedrooms, square footage, the location, and the age of the property. If we have a dataset of customers, features could include age, gender and occupation.\n",
    "\n",
    "Features can be:\n",
    "- Quantitative/qualitative\n",
    "- Visible/latent\n",
    "- Deterministic/statistical\n",
    "\n",
    "Why do we do feature engineering?\n",
    "1) To reduce the complexity of the data\n",
    "2) To identify relevant features or design meaningful transformations (requires domain expertise)\n",
    "\n",
    "Typucally, in deep learning architectures, features are automatically extracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative features can have varying magnitudes. Feature scaling is the process of normalising the values of the features in your dataset. This means that no feature dominates or skews the model too much. This helps improve model convergence. It also means that we can input features into the model as a single vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Max Scaling (Normalisation)**\n",
    "- All features range from **0 to 1**\n",
    "- Useful when distribution of the data is unknown (or heavily non-Gaussian)\n",
    "- Sensitive to outliers\n",
    "- May not preserve the relationship between datapoints\n",
    "\n",
    "$$\n",
    "x_{scaled, j} = \\frac{x_j - \\min{(\\mathbf{x})}}{\\max{(\\mathbf{x})} - \\min{(\\mathbf{x})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardisation**\n",
    "- Transforming the distribution into **zero mean, unit variance**\n",
    "- Variables are not restricted (-$\\infty$, +$\\infty$)\n",
    "- Useful when disttribution is similar to or is Gaussian\n",
    "- Not sensitive to outliers\n",
    "- Preserves the relationships between datapoints\n",
    "\n",
    "$$\n",
    "x_{standard, j} = \\frac{x_{j} - \\text{mean}(\\mathbf{x})}{\\text{std}(\\mathbf{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Learning Strategies**\n",
    "\n",
    "Classification -> *predicting a categorical variable*\n",
    "- Naive bayes\n",
    "- Artificial neural networks\n",
    "- Support vector machines\n",
    "- Decicision trees\n",
    "\n",
    "Regression -> *predicting a numerical value*\n",
    "- Linear regression\n",
    "- Gaussian process\n",
    "\n",
    "\n",
    "**Unsupervised Learning Strategies**\n",
    "\n",
    "Clustering -> *grouping a set of objects*\n",
    "- K means\n",
    "- Hierarchical clustering\n",
    "- Gaussian mixture\n",
    "\n",
    "Association -> *discovering relations between variables in large datasets*\n",
    "- Autoencoders\n",
    "- Artifical neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data-driven model is parameterised in some way. During training, we learn the values of these parameters using the dataset. This is an optimisation problem because we need to find the values of the parameters that minimises the model prediction error. \n",
    "\n",
    "Note, the MSE is good for small error values. When you have larger errors, the squared term magnifies their impact, which accelerates the minimisation process during training. However, it also amplifies the effect of outliers. \n",
    "\n",
    "The objective/cost/loss function tells us the error in a model. This function must be chosen well to respresent the design goals. The proccess of minimising error and finding the optimal model parameters is known as **training**.\n",
    "\n",
    "One way of computing the error is finding the **mean squared error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{N} \\sum_{i=1}^{N} (z_i - y_i(\\mathbf{p}))\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $z_i$ is the true output for datapoint i\n",
    "- $y_i$ is the model output for datapoint i\n",
    "- $\\mathbf{p}$ is the parameters of the model\n",
    "- N is the number of datapoints\n",
    "\n",
    "Some alternative loss functions for regression inclead mean absolute error (MAE) and root mean squared error (RMSE). For classification, some examples of loss functions are cross-entropy, ROC-AUC and accuracy.\n",
    "\n",
    "Increasing the model complexity doesn't necessarily mean the model will be better, due to the risk of underfitting or overfitting. This idea is known as **Occam's razor**. If you underfit, there is low accuracy in training and validation. We can tackle underfitting by increasing model complexity, more iterations or changing the learning rate. If you overfit, there is high accuracy in training and low accuracy in validation. We can tackle overfitting using regularisation, feature selection or early stopping.\n",
    "\n",
    "Note, classification doesn't have learning curves usually. The performance of the model at each iteration is computed differently. Some examples of error metrics are a confusion matrix, classification score or computing the number of false positives and false negatives.\n",
    "\n",
    "Once we have a model with optimal parameters. We can test its performance using test data. This phase is known as the **testing** phase. \n",
    "\n",
    "The learning curve is the graph of the loss at each iteration of the optimisation scheme. Ideally, this should converge to around 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
